# Daily Project Resumption Prompt Template
## AI Language Tutor App - Phase 3A IN PROGRESS

**Last Updated**: 2025-10-31 (Session 4 Complete - MAJOR SUCCESS!)  
**Current Phase**: ðŸš€ Phase 3A - Comprehensive Testing (IN PROGRESS)  
**Current Status**: 12 modules COMPLETE (7 at 100%, 2 at 96%, 1 at 76%, 1 at 35%) âœ…  
**Next Task**: Continue Phase 3A - AI Services and Processors

---

## Standardized Daily Startup Prompt

**âš¡ COPY AND PASTE THIS INTO NEW CHAT SESSION:**

---

**DAILY PROJECT RESUMPTION - AI Language Tutor App Phase 3A**

Hello! I'm resuming work on the AI Language Tutor App, currently in Phase 3A (Comprehensive Testing).

**PROJECT CONTEXT**:
- **Phase**: Phase 3A - Comprehensive Testing (IN PROGRESS - EXCEPTIONAL momentum!)
- **Achievement**: 12 modules tested, 4 completed in Session 4!
- **Modules at 100%**: 7 (scenario_models, sr_models, conversation_models, conversation_manager, conversation_state, conversation_messages, conversation_analytics)
- **Modules at >90%**: 2 (progress_analytics 96%, auth 96%)
- **Modules at >70%**: 1 (scenario_manager 76%)
- **Overall Coverage**: 48% (up from baseline 44%, targeting >70% critical modules)
- **Tech Stack**: FastAPI + FastHTML + multi-LLM routing + Mistral STT + Piper TTS + SQLite/ChromaDB/DuckDB

**ðŸ”¥ CRITICAL PROJECT PHILOSOPHY**:
> "Quality and performance above all."
> "Time is not a restriction - let's keep the pace."
> "Test coverage > 90% is the minimum bar to move forward."
> "We have plenty of time to do this right."

**ðŸš¨ MANDATORY FIRST STEP - READ STATUS FILES**:
Before ANY work, you MUST read these files IN THIS ORDER:

1. **`docs/PHASE_3A_PROGRESS.md`** â­ START HERE
   - Real-time Phase 3A progress tracker
   - 12 modules completed with full details
   - Coverage statistics and lessons learned
   - Next module priorities

2. **`docs/SESSION_4_SUMMARY.md`** â­ LATEST SESSION
   - Session 4 MAJOR achievements
   - 4 modules completed (conversation_messages, conversation_analytics, scenario_manager, user_management)
   - 123 tests created, 2,423 lines of test code
   - Comprehensive results and recommendations

3. **`docs/SESSION_3_CONTINUED_FINAL_SUMMARY.md`** â­ PREVIOUS SESSION
   - Session 3 Continued achievements for context
   - Phase 3A.6 and 3A.7 details
   - Testing patterns established
   - Technical insights and process lessons

4. **`docs/PROJECT_STATUS.md`** â­ PROJECT OVERVIEW
   - Overall project status
   - All phase summaries
   - Critical metrics

**ðŸ“‹ PLEASE PERFORM THESE STEPS IN ORDER**:

1. **Load Current Status** (5-10 minutes)
   - Read `docs/PHASE_3A_PROGRESS.md` - Check detailed progress
   - Read `docs/SESSION_3_CONTINUED_FINAL_SUMMARY.md` - Latest achievements
   - Read `docs/PROJECT_STATUS.md` - Overall context

2. **Verify Environment** (2-3 minutes)
   - Run validation: `./ai-tutor-env/bin/python scripts/validate_environment.py` (5/5 checks must pass)
   - Check git status: `git status` (should be clean)
   - Verify tests passing: `./ai-tutor-env/bin/python -m pytest tests/ -q` (323+ tests should pass)
   - Check overall coverage: `./ai-tutor-env/bin/python -m pytest --cov=app --cov-report=term -q | tail -40`

3. **Select Next Module for Phase 3A.9** (Strategic decision)
   
   **Recommended Priority Order**:
   
   **Option A: Continue Conversation Stack (Recommended)**
   - **conversation_messages.py** (39% coverage, ~58 uncovered lines)
     - Pros: Medium effort, completes conversation module ecosystem
     - Builds on conversation_manager + conversation_state work
   
   - **conversation_analytics.py** (27% coverage, ~35 uncovered lines)
     - Pros: Medium effort, finishes conversation services
     - Learning insights generation
   
   **Option B: High-Value Services**
   - **scenario_manager.py** (23% coverage, ~182 uncovered lines)
     - Pros: Scenario-based learning is core feature
     - Cons: Larger module, more effort
   
   **Option C: Critical User Module (High Effort)**
   - **user_management.py** (12% coverage, ~274 uncovered lines)
     - Pros: Absolutely critical module
     - Cons: Complex, database-heavy, will take significant time
     - Recommendation: Save for when we have a longer session

4. **Testing Strategy** (Follow established patterns)
   
   **Patterns Established**:
   - **Security Testing** (auth.py): Comprehensive coverage of authentication flows
   - **Facade Testing** (conversation_manager.py): Test delegation, not implementation
   - **State Management** (conversation_state.py): Test complete lifecycle
   
   **Best Practices**:
   - Use descriptive test class names: `TestMethodNameOrFeature`
   - 2-5 tests per method: happy path, edge cases, error handling
   - Mock external dependencies (DB, API calls, other services)
   - Test both success and failure paths
   - Validate error messages and types
   - Use side effects for stateful operations (DB loads)
   
   **Test Commands**:
   ```bash
   # Run tests for specific file
   ./ai-tutor-env/bin/python -m pytest tests/test_MODULE_NAME.py -v
   
   # Check coverage for specific module
   ./ai-tutor-env/bin/python -m pytest tests/test_MODULE_NAME.py --cov=app.services.MODULE_NAME --cov-report=term-missing
   ```

5. **Track Progress** (Maintain momentum)
   - Commit after reaching coverage target (usually 90%+)
   - Update `docs/PHASE_3A_PROGRESS.md` after each module
   - Create session handover at end if context limit approaching
   - Keep commits atomic with clear messages

**CRITICAL REQUIREMENTS**:
- ðŸš¨ **ALWAYS** use the virtual environment: `./ai-tutor-env/bin/python`
- ðŸš¨ **ALWAYS** run tests after adding them to verify they pass
- ðŸš¨ **TARGET**: >90% coverage minimum per module (100% is achievable!)
- âŒ DO NOT skip error handling tests
- âŒ DO NOT assume function signatures - read the code
- âœ… DO follow established testing patterns
- âœ… DO use comprehensive mocking strategies
- âœ… DO test both success and error paths
- âœ… DO maintain clear test organization

**CURRENT PROGRESS SNAPSHOT**:
```
Phase 3A Progress: Strong Momentum! ðŸš€

Modules Completed (8 total):
âœ… progress_analytics_service.py: 78% â†’ 96% (+18%) - 12 tests
âœ… scenario_models.py: 92% â†’ 100% (+8%) - 17 tests
âœ… sr_models.py: 89% â†’ 100% (+11%) - 20 tests
âœ… conversation_models.py: 99% â†’ 100% (+1%) - 15 tests
âœ… auth.py: 60% â†’ 96% (+36%) - 63 tests â­ CRITICAL
âœ… conversation_manager.py: 70% â†’ 100% (+30%) - 24 tests
âœ… conversation_state.py: 58% â†’ 100% (+42%) - 22 tests

Statistics:
- Total tests: 323+ passing (0 failing, 0 skipped)
- Modules at 100%: 5
- Modules at >90%: 2
- Overall coverage: ~46% (target >90%)
- Test lines written: 3,672+ lines

Quality Metrics:
- All tests passing âœ…
- Comprehensive coverage âœ…
- Excellent documentation âœ…
- Clean git history âœ…
```

**SESSION GOALS**:
1. Select next module (recommendation: conversation_messages.py or conversation_analytics.py)
2. Create comprehensive test file
3. Achieve >90% coverage (aim for 100%)
4. Commit and document progress
5. Update trackers

**Next Modules to Consider**:
- conversation_messages.py (39%) - Medium effort, completes conversation stack
- conversation_analytics.py (27%) - Medium effort, learning insights
- scenario_manager.py (23%) - High value, larger effort
- user_management.py (12%) - Critical but complex, save for later

---

## Quick Reference Commands

**Validation & Testing**:
```bash
# Validate environment
./ai-tutor-env/bin/python scripts/validate_environment.py

# Run all tests (quick)
./ai-tutor-env/bin/python -m pytest tests/ -q

# Run all tests (verbose)
./ai-tutor-env/bin/python -m pytest tests/ -v

# Run specific test file
./ai-tutor-env/bin/python -m pytest tests/test_MODULE_NAME.py -v

# Check coverage for specific module
./ai-tutor-env/bin/python -m pytest tests/test_MODULE_NAME.py --cov=app.services.MODULE_NAME --cov-report=term-missing

# Check overall project coverage
./ai-tutor-env/bin/python -m pytest tests/ --cov=app --cov-report=term -q | tail -40

# Generate HTML coverage report
./ai-tutor-env/bin/python -m pytest tests/ --cov=app --cov-report=html
# Then open: htmlcov/index.html
```

**Git Workflow**:
```bash
# Check status
git status

# Stage and commit
git add tests/test_MODULE_NAME.py
git commit -m "âœ… Phase 3A.X: Achieve Y% coverage for MODULE_NAME (X% to Y%)"

# Update docs
git add docs/PHASE_3A_PROGRESS.md
git commit -m "ðŸ“Š Update Phase 3A progress: 3A.X complete (MODULE_NAME Y%)"

# Push to remote
git push origin main
```

**Coverage Analysis**:
```bash
# Check specific module coverage
./ai-tutor-env/bin/python -m pytest tests/ --cov=app.services.MODULE_NAME --cov-report=term-missing -q

# Get coverage for multiple modules
./ai-tutor-env/bin/python -m pytest tests/ --cov=app.services --cov-report=term -q | grep "app/services"
```

---

## Testing Patterns Reference

### Pattern 1: Facade Testing (conversation_manager.py)
```python
# Test delegation, not implementation
with patch.object(self.manager.state_manager, "method", new_callable=AsyncMock) as mock:
    mock.return_value = expected_value
    result = await self.manager.method(args)
    mock.assert_called_once_with(expected_args)
```

### Pattern 2: State Management (conversation_state.py)
```python
# Use side effects for stateful operations
async def mock_load_side_effect(id):
    self.manager.active_conversations[id] = mock_context
    return True

with patch.object(self.manager, "_load_from_db") as mock:
    mock.side_effect = mock_load_side_effect
    result = await self.manager.resume(id)
```

### Pattern 3: Security Testing (auth.py)
```python
# Test complete flows with real service instances
auth = AuthenticationService()
token = auth.create_access_token(user_data)
result = auth.verify_token(token)
# Validate security properties
```

---

## End of Prompt Template
